{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jgbtj1hw7Ayh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1** .What is Simple Linear Regression (SLR)? Explain its purpose.\n",
        "A statistical technique called simple linear regression (SLR) uses a straight line to represent the relationship between one independent variable (X) and one dependent variable (Y).\n",
        "\n",
        " Goal:\n",
        "\n",
        " to forecast Y's value using X.\n",
        "\n",
        " to comprehend the direction and intensity of the relationship between X and Y.\n",
        "\n",
        " to determine which line best fits the data while minimizing prediction error.\n"
      ],
      "metadata": {
        "id": "apIWA8Ko7IDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2**. What are the key Assumptions of Simple Linear Regression:**\n",
        "\n",
        "For the results of Simple Linear Regression to be reliable and valid, several key assumptions about the data and the relationship between the variables must be met:\n",
        "\n",
        "1.  **Linearity:** The relationship between the independent variable ($x$) and the dependent variable ($y$) must be linear. This means that the data points should roughly follow a straight line when plotted.\n",
        "2.  **Independence of Errors:** The errors (residuals) should be independent of each other. This means that the error for one observation should not be related to the error for any other observation. This assumption is often violated in time series data or when there are dependencies between observations.\n",
        "3.  **Homoscedasticity (Constant Variance of Errors):** The variance of the errors should be constant across all levels of the independent variable. In other words, the spread of the residuals should be roughly the same for all predicted values. If the variance of the errors increases or decreases as the independent variable changes, it is called heteroscedasticity.\n",
        "4.  **Normality of Errors:** The errors (residuals) should be normally distributed. This means that if you were to plot a histogram of the residuals, it should resemble a bell curve. While this assumption is less critical for large sample sizes due to the Central Limit Theorem, it is important for hypothesis testing and confidence intervals.\n",
        "5.  **No Multicollinearity (for multiple regression, but relevant to consider in SLR with dummy variables):** While strictly an assumption for multiple linear regression, it's worth noting that in SLR, the independent variable should not be perfectly correlated with any other variable in the model (though in SLR, there's only one independent variable).\n",
        "\n",
        "Violations of these assumptions can lead to biased estimates, incorrect standard errors, and invalid statistical inferences. It's important to check these assumptions when building and evaluating an SLR model."
      ],
      "metadata": {
        "id": "ouvgJTo77lqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3**. Write the mathematical equation for a simple linear regression model and expalin each term.\n",
        "\n",
        "The mathematical equation for a simple linear regression model is:\n",
        "\n",
        "$y = \\beta_0 + \\beta_1 x + \\epsilon$\n",
        "\n",
        "Let's break down each term:\n",
        "\n",
        "*   **$y$**: This is the **dependent variable** (also called the response variable). It's the variable you are trying to predict or explain.\n",
        "*   **$\\beta_0$**: This is the **y-intercept** (also called the constant term or bias term). It represents the predicted value of $y$ when the independent variable $x$ is equal to 0.\n",
        "*   **$\\beta_1$**: This is the **slope** (also called the regression coefficient). It represents the change in the dependent variable $y$ for a one-unit increase in the independent variable $x$. It indicates the strength and direction of the linear relationship between $x$ and $y$.\n",
        "*   **$x$**: This is the **independent variable** (also called the predictor variable or feature). It's the variable you are using to predict the value of $y$.\n",
        "*   **$\\epsilon$**: This is the **error term** (also called the residual). It represents the difference between the actual value of $y$ and the value of $y$ predicted by the linear model. It accounts for the variability in $y$ that is not explained by the linear relationship with $x$. The error term is assumed to be random and follow a specific distribution (often normal) with a mean of zero and constant variance."
      ],
      "metadata": {
        "id": "Ch1Rlszz8Fyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4**. Provide a real-world example where simple linear regression can be applied.\n",
        " **Real-World Example of Simple Linear Regression:**\n",
        "\n",
        "A real-world example is predicting **salary** (dependent variable) based on **years of experience** (independent variable). Simple linear regression can show the relationship's strength and direction, estimate the average salary increase per year of experience, and predict salary based on experience. This is useful for salary benchmarking and understanding career growth, although other factors also influence salary."
      ],
      "metadata": {
        "id": "0JIcTvn08UqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5**. What is the method of least squares in linear regression?\n",
        "**Method of Least Squares in Linear Regression:**\n",
        "\n",
        "To determine the best-fit line for a collection of data points in linear regression, the method of least squares is employed.\n",
        "\n",
        " How it operates:\n",
        "\n",
        " Determine the difference between the line's predicted value (Ï¶) and the actual value (Y) for each data point.\n",
        " We refer to this discrepancy as the residual.\n",
        "\n",
        " To prevent positive and negative errors from canceling out, square each residual.\n",
        "\n",
        " The sum of squared errors (SSE) is obtained by adding all of the squared residuals together.\n",
        "\n",
        " The line that minimizes this total squared error is then determined by the method (i.e., slope m and intercept c).\n",
        "\n",
        " Why we employ it:\n",
        "\n",
        " because it lowers the overall prediction error and provides the most stable and accurate line.\n",
        "\n",
        " It guarantees that the selected line closely matches the data."
      ],
      "metadata": {
        "id": "2ITqPOBw8cfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6**. What is Logistic Regression? How does it differ from Linear Regression?\n",
        "Logistic Regression:**\n",
        "\n",
        "Logistic Regression is a statistical model used for **binary classification tasks**. It predicts the **probability** that an observation belongs to a particular category or class when the dependent variable has two possible outcomes (e.g., spam/not spam). It uses a **sigmoid function** to map the linear combination of independent variables to a probability value between 0 and 1.\n",
        "\n",
        "**How it differs from Linear Regression:**\n",
        "\n",
        "The key differences are in their **purpose** and the **type of dependent variable** they handle:\n",
        "\n",
        "1.  **Purpose:** Linear Regression predicts a continuous outcome, while Logistic Regression predicts the probability of a binary outcome (classification).\n",
        "2.  **Dependent Variable:** Linear Regression's dependent variable is continuous; Logistic Regression's is categorical and binary.\n",
        "3.  **Output:** Linear Regression outputs a continuous value; Logistic Regression outputs a probability between 0 and 1.\n",
        "4.  **Mathematical Function:** Linear Regression uses a linear equation; Logistic Regression uses a sigmoid function applied to a linear combination of independent variables.\n",
        "\n",
        "In essence, Linear Regression is for predicting numerical values, while Logistic Regression is for classifying observations into one of two groups based on probability."
      ],
      "metadata": {
        "id": "Qbyc_sPS9MOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ANtXZqD29ZGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7**. Name and briefly describe three common evaluation metrics for regression models.\n",
        "\n",
        "**Common Evaluation Metrics for Regression Models:**\n",
        "\n",
        "Evaluating the performance of a regression model is crucial to understand how well it fits the data and makes predictions. Here are three common evaluation metrics:\n",
        "\n",
        "1.  **Mean Absolute Error (MAE):**\n",
        "    *   **Description:** MAE is the average of the absolute differences between the actual values and the predicted values. It measures the average magnitude of the errors in a set of predictions, without considering their direction.\n",
        "    *   **Formula:** $ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $\n",
        "    *   **Interpretation:** A lower MAE indicates a better model performance. It is less sensitive to outliers compared to MSE.\n",
        "\n",
        "2.  **Mean Squared Error (MSE):**\n",
        "    *   **Description:** MSE is the average of the squared differences between the actual values and the predicted values. It penalizes larger errors more heavily than smaller errors due to the squaring.\n",
        "    *   **Formula:** $ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
        "    *   **Interpretation:** A lower MSE indicates a better model performance. It is more sensitive to outliers than MAE. The square root of MSE is the Root Mean Squared Error (RMSE), which is often used because it is in the same units as the dependent variable.\n",
        "\n",
        "3.  **R-squared ($R^2$) - Coefficient of Determination:**\n",
        "    *   **Description:** $R^2$ measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It indicates how well the regression model fits the observed data.\n",
        "    *   **Formula:** $ R^2 = 1 - \\frac{SSE}{SST} $ where SSE is the sum of squared errors (residuals) and SST is the total sum of squares.\n",
        "    *   **Interpretation:** $R^2$ ranges from 0 to 1. An $R^2$ of 0 means the model explains none of the variance in the dependent variable, while an $R^2$ of 1 means the model explains all of the variance. A higher $R^2$ generally indicates a better fit, but it's important to consider other metrics and the context of the problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "OyFo98V8-dIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**8**. What is the purpose of the R-squared metric in regression analysis?\n",
        "\n",
        "**Purpose of the R-squared ($R^2$) Metric:**\n",
        "\n",
        "The primary purpose of the R-squared ($R^2$) metric in regression analysis is to measure the **proportion of the variance in the dependent variable that is predictable from the independent variable(s)**.\n",
        "\n",
        "In simpler terms, $R^2$ tells us how well the independent variable(s) in our model explain the variability in the dependent variable.\n",
        "\n",
        "Here's a breakdown of its purpose:\n",
        "\n",
        "*   **Goodness of Fit:** $R^2$ serves as an indicator of the \"goodness of fit\" of the regression model. A higher $R^2$ value suggests that the model fits the observed data better.\n",
        "*   **Explained Variance:** It quantifies the percentage of the total variation in the dependent variable that is accounted for by the linear relationship with the independent variable(s).\n",
        "*   **Interpretation:**\n",
        "    *   An $R^2$ of 0 means that the model explains none of the variability in the dependent variable.\n",
        "    *   An $R^2$ of 1 means that the model explains all of the variability in the dependent variable.\n",
        "    *   An $R^2$ between 0 and 1 indicates the proportion of variance explained. For example, an $R^2$ of 0.75 means that 75% of the variation in the dependent variable can be explained by the independent variable(s) in the model.\n",
        "\n",
        "It's important to note that while a high $R^2$ is desirable, it doesn't necessarily mean the model is perfect or that there isn't a better model. $R^2$ can be influenced by the number of independent variables (especially in multiple regression), and it doesn't tell us about the validity of the model's assumptions or whether the relationships are causal. Therefore, $R^2$ should be considered along with other evaluation metrics and domain knowledge."
      ],
      "metadata": {
        "id": "fS3EbbzC-wI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**9**. Write Python code to fit a simple linear regression model using scikit-learn and print the slope intercept.\n",
        "\n"
      ],
      "metadata": {
        "id": "WL10wkY-A2dL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Independent variable (features)\n",
        "y = np.array([2, 4, 5, 4, 5])  # Dependent variable (target)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the slope and intercept\n",
        "print(\"Slope:\", model.coef_[0])\n",
        "print(\"Intercept:\", model.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxIKD9tzBJNC",
        "outputId": "c05c996e-1e18-4bc8-b94f-af70d9cdd291"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope: 0.6\n",
            "Intercept: 2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**10**.  How do you interpret the coefficients in a simple linear regression model?\n",
        "**Interpreting Coefficients in Simple Linear Regression:**\n",
        "\n",
        "In a simple linear regression model with the equation:\n",
        "\n",
        "$y = \\beta_0 + \\beta_1 x + \\epsilon$\n",
        "\n",
        "The coefficients $\\beta_0$ and $\\beta_1$ have specific interpretations:\n",
        "\n",
        "1.  **Intercept ($\\beta_0$):**\n",
        "    *   **Interpretation:** The intercept represents the predicted mean value of the dependent variable ($y$) when the independent variable ($x$) is equal to zero.\n",
        "    *   **Context is Key:** It's important to consider if an independent variable value of zero is meaningful in the context of your data. If zero is outside the range of your observed data or is not a plausible value, the intercept might not have a practical interpretation on its own.\n",
        "\n",
        "2.  **Slope ($\\beta_1$):**\n",
        "    *   **Interpretation:** The slope represents the estimated change in the mean of the dependent variable ($y$) for a one-unit increase in the independent variable ($x$), assuming all other variables are held constant (although in simple linear regression, there's only one independent variable).\n",
        "    *   **Direction and Magnitude:**\n",
        "        *   A **positive** slope ($\\beta_1 > 0$) indicates a positive linear relationship: as $x$ increases, $y$ tends to increase.\n",
        "        *   A **negative** slope ($\\beta_1 < 0$) indicates a negative linear relationship: as $x$ increases, $y$ tends to decrease.\n",
        "        *   The **magnitude** of the slope indicates the strength of the relationship: a larger absolute value of $\\beta_1$ suggests a steeper slope and a stronger impact of $x$ on $y$.\n",
        "\n",
        "**Example using the previous code:**\n",
        "\n",
        "In the previous code example where we predicted a target variable based on a single feature:\n",
        "\n",
        "*   Slope: 0.6\n",
        "*   Intercept: 2.2\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "*   **Intercept (2.2):** When the independent variable (feature) is 0, the predicted value of the dependent variable (target) is 2.2.\n",
        "*   **Slope (0.6):** For every one-unit increase in the independent variable (feature), the predicted value of the dependent variable (target) increases by 0.6.\n",
        "\n",
        "Understanding these interpretations is crucial for drawing meaningful conclusions from your simple linear regression model and communicating the relationship between your variables."
      ],
      "metadata": {
        "id": "eThlUDIbBUpD"
      }
    }
  ]
}